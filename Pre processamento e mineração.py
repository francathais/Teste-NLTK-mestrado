# -*- coding: utf-8 -*-
"""mestrado analise linguagem natural

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rZNlD-B7Sac3_1XZ9Hfk01zdMY1a_sfN
"""

pip install nltk #instalando o nltk

import nltk 
import re 
import string
import pandas as pd
import numpy as np  
#importa

nltk.download ('all') #baixa tudo no nltk

from nltk.tokenize import sent_tokenize
nltk.download('punkt')
from nltk.tokenize import word_tokenize

#subindo no google colab
from google.colab import files
uploaded = files.upload()

#fazendo a leitura do arquivo
f = open('mestrado.txt', 'r')
texto = ''
while 1:
    line = f.readline()
    if not line: break
    texto += line
f.close()

print(texto)
print(type(texto))

f #meu arquivo

texto #meu texto

"""#Limpeza

remoção das pontuações tirando maiusculas e minusculas
"""

def processamento (texto): 
 texto_formatado=texto.lower() 
 return texto_formatado

texto_formatado=processamento(texto)
texto_formatado

"""# Tokenization 

Transforma elementos do seu texto em tokens, ou seja, strings dentro de uma lista

É o nome dado para o processo de dividir uma grande quantidade de texto em pequenas quantidades - essas pequenas quantidades são chamadas de tokens. Essa é uma divisão importante para fazer análises textuais.

O nltk possui um módulo chamado tokenize que facilita o processo de divisão de um texto. A função word_tokenize() divide um texto por palavras e pontuações:
"""

# Quebra em palavras
tokenized_word = word_tokenize(texto_formatado)
print(tokenized_word)

# Aqui também deixamos as palavras em letras minúsculas
tokenized_word2 = [w.lower() for w in tokenized_word]
print(tokenized_word2)

"""#Remover Stopwords
Existem palavras na construção textual chamadas de stopwords, tais palavras, dentro de uma abordagem de NLP, são irrelevantes e sua remoção colaboram com a analise textual. Alguns exemplos de stopwords comuns no português são preposições (em, na, no, etc), artigos (a, o,os, etc),conjunções (e, mas, etc), entre outras.
"""

#1)importar analise de stopwords e também visualizar as existentes
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('portuguese'))
print(stop_words)

#2) aqui eu tiro o que nao quero
stop_words.update(('uso', 'né', '/','i',',',';','l','.',':', '***','-','–','|', '!', '?','gente','aqui', 'então', 'pra',
                   'porque', 'acho', 'assim', 'anos', 'coisa','tudo', 'muita', 'hoje','lá', 'ali', 'vai', 'todo','campo',
                   'agora','sobre', 'outro', 'espaço'))
print(stop_words)

#3) Remover essas stopwords
tokenized_word_3 = []
for w in tokenized_word2:
    if w not in stop_words:
        tokenized_word_3.append(w)

tokenized_word_3

# Analisa a distribuição das palavras
from nltk.probability import FreqDist
fdist = FreqDist(tokenized_word_3)
fdist.most_common(11)

# Gráfico de distribuiçao de palavras
import matplotlib.pyplot as plt
plt.figure()
fdist.plot(11,cumulative=False)

#Outra forma de ver duas palavras mais comuns
Comum=fdist.most_common(2)
Comum

"""
#Outras analises"""

#quantas vezes a palavra repete
texto.count('pessoas')

tokenized_word_3.count('pessoas')

tokenized_word_3.count('mulheres')

personagens = ['lazer', 'cultura', 'esporte', 'trabalho', 'dinheiro', 'escola',
              'transporte', 'saúde']
for p in personagens:
 print('{}: {}'.format(p, tokenized_word_3.count(p)))

"""#Possibilidade de gráficos"""

#Grafico de linha
import matplotlib.pyplot
personagens = ['pessoas', 'território', 'comunidade','fazer']
valores = [21, 9, 9, 9]
matplotlib.pyplot.ylim(0, 30)
matplotlib.pyplot.plot(personagens,valores)
matplotlib.pyplot.show()

personagens = ['paraisópolis', 'pessoas', 'território', 'comunidade','fazer',
                 'pandemia', 'mãe', 'cultura', 'lazer', 'difícil']
valores = [38, 21, 9, 9, 9, 9, 8, 8, 8, 7]

#grafico barras horizontais 
plt.barh(personagens,valores, color='black') 
plt.ylabel('Palavras citadas',fontsize=12) 
plt.xlabel('Valores',fontsize=12)
plt.title('Citações mais frequentes no discuso',fontsize=12)
plt.show()

#grafico barras horizontais com definições de espaço e titulo 
figura = plt.figure()
ax = figura.add_axes([0,0,1.0,1])
personagens = ['paraisópolis', 'pessoas', 'território',
                 'fazer', 'pandemia', 'mãe']
valores = [38, 21, 9, 9, 9, 8]
ax.barh(personagens, valores, color = "black")
ax.set_title("Citações mais frequentes nas entrevistas", fontsize=14)
ax.set_xlabel("Valores", fontsize=14)
ax.set_ylabel("Palavras citadas", fontsize=14)
plt.show()

#grafico de barras
figura = plt.figure()
ax = figura.add_axes([0,0,1.1,1.0])
personagens = ['paraisópolis', 'pessoas', 'território',
                 'fazer', 'pandemia', 'mãe']
valores = [38, 21, 9, 9, 9, 8]
ax.bar(personagens, valores, color = "black")
ax.set_title("Citações mais frequentes nas entrevistas", fontsize=14)
ax.set_xlabel("Palavras citadas", fontsize=13)
ax.set_ylabel("Valores", fontsize=13)
plt.show()

"""#Possibilidade de tabelas"""

#feia
citou_1 = pd.Series({'Palavra citada': 'terra', 'Repetições': 7})
citou_2 = pd.Series({'Palavra citada': 'pessoas', 'Repetições': 3})
citou_3 = pd.Series({'Palavra citada': 'bem-estar', 'Repetições': 2})
citou_4 = pd.Series({'Palavra citada': 'milhões', 'Repetições': 2})
citou_5 = pd.Series({'Palavra citada': 'seca', 'Repetições': 2})
citou_6 = pd.Series({'Palavra citada': 'mudanças', 'Repetições': 2})
citou_7 = pd.Series({'Palavra citada': 'climáticas', 'Repetições': 2})
citou_8 = pd.Series({'Palavra citada': 'restauração', 'Repetições': 2})
df = pd.DataFrame([citou_1, citou_2, citou_3, citou_4, citou_5, citou_6, citou_7, citou_8])
df


#bonita
import plotly.graph_objects as go 
  
fig = go.Figure(data=[go.Table( 
    header=dict(values=['Citações no discurso', 'Repetições']), 
    cells=dict(values=[['Lazer', 'Cultura', 'Trabalho', 'Dinheiro', 'Transporte',
         'Escola', 'Saúde', 'Esporte'], 
                       [8, 8, 5, 5, 5, 4, 2, 1]])) 
]) 
fig.update_layout(width=950, height=950)
fig.show()

"""#Analise de sentimentos (LSTM)"""

from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import LabelEncoder
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from keras.preprocessing.text import Tokenizer 
from keras.preprocessing.sequence import pad_sequences 
from keras.utils import np_utils

#a variavel tokenized_word_3 são as PALAVRAS sem stopwords e separadas, 
#para fazer analise de sentmentos a seguir usa-se a texto_formatado que será dividida em SENTENÇAS com line_tokenize
from nltk.tokenize import line_tokenize
sentimentos= line_tokenize(texto_formatado)

#ver sentimentos
sentimentos

#transformo as sentenças em um DtaFrame (tabela) com uma unica coluna #Frases
my_list = sentimentos
df = pd.DataFrame(my_list, columns = ['Frases'])
print(df)

#insere a segunda coluna #Sentimentos na tabela feita acima
dfComDuasColunas = pd.DataFrame(df,columns=['Frases']) 
Sentimento = ['Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Positivo', 'Negativo',
              'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Positivo', 'Negativo', 'Positivo',
              'Positivo', 'Positivo', 'Neutro', 'Positivo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo',
              'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo',
              'Negativo','Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Positivo', 'Negativo',
              'Negativo', 'Positivo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo', 'Negativo']
dfComDuasColunas['Sentimento'] = Sentimento
print(dfComDuasColunas)

#vendo o tamanho das colunas da tabela
dfComDuasColunas.groupby(['Sentimento']).size()

"""Rede neural recorrente

"""

#Processamento natural precisa que a tabela esteja em numero, sendo que Fases é onde está o meu texto

#cria o modelo com numero de palavras (tokens) que será criado, aqui nao executa
token=Tokenizer(num_words=100)
token.fit_on_texts(dfComDuasColunas['Frases'].values)

#Usamos objeto token e passamos na variavel Frases
#Transformando a coluna Frases em numeros para a rede neural
X=token.texts_to_sequences(dfComDuasColunas['Frases'].values)
X=pad_sequences(X, padding="post", maxlen=100)
X

#Transformando a coluna Sentimento em numeros para a rede neural
labelencoder=LabelEncoder() 
Y=labelencoder.fit_transform(dfComDuasColunas['Sentimento'])
Y

#Dividimos o teste de variavel dependente  eidependente
Y=np_utils.to_categorical(Y)
print(Y)

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1) 
X_test

#Gerando o modelo compilado de rede neural 

modelo=Sequential()
modelo.add(Embedding(input_dim=len(token.word_index),output_dim=128,input_length=X.shape[1]))
modelo.add(SpatialDropout1D(0.2))
modelo.add(LSTM(units=196, dropout=0.2, recurrent_dropout=0,
                activation='tanh', recurrent_activation='sigmoid', unroll=False, use_bias=True))
modelo.add(Dense(units=3, activation="softmax"))


modelo.compile(loss='categorical_crossentropy', optimizer="adam", metrics=['accuracy']) 
print(modelo.summary())

#Outro modelo de rede neural

modelo.fit(X_train, Y_train, epochs=10, batch_size=30, verbose=True, validation_data=(X_test, Y_test))

loss, accuracy=modelo.evaluate(X_test, Y_test) 
print("Loss:", loss)
print("Accuracy:", accuracy)

prev=modelo.predict(X_test)
print(prev)

#Modelo de previsao

prev=modelo.predict(X_test) 
print(prev)

#chance de cada item ser 0

"""#VADER

vê o sentimento maior na fala
"""

nltk.download("vader_lexicon")

from nltk.sentiment.vader import SentimentIntensityAnalyzer 
s = SentimentIntensityAnalyzer()

a = 'lies'
x=mas.polarity_scores(a)
print(x)

"""vê o sentimento maior na fala em pt"""